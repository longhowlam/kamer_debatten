{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595760049401",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"OpenAIGPTLMHeadModel\")\n",
    "model = BertModel.from_pretrained(\"wietsedv/bert-base-dutch-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/wietsedv/bert-base-dutch-cased?text=gaan+we+%5BMASK%5D+\n",
    "\n",
    "https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Your max_length is set to 142, but you input_length is only 17. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'summary_text': ' Sam Shleifer writes the best docstring examples in the world .'}]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model = `'bert-base-multilingual-uncased')\n",
    "summarizer(\"Sam Shleifer writes the best docstring examples in the whole world.\", min_length=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at wietsedv/bert-base-dutch-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForMaskedLM were not initialized from the model checkpoint at wietsedv/bert-base-dutch-cased and are newly initialized: ['cls.predictions.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer_NL = AutoTokenizer.from_pretrained(\"wietsedv/bert-base-dutch-cased\")\n",
    "model_NL = AutoModelWithLMHead.from_pretrained(\"wietsedv/bert-base-dutch-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Your max_length is set to 20, but you input_length is only 7. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'summary_text': 'Ik ga nu huis naar mijn werk'}]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model = model_NL, tokenizer=tokenizer_NL)\n",
    "summarizer(\"Ik ga nu nmaar huis\", min_length=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 12.7 s, sys: 919 ms, total: 13.6 s\nWall time: 13.8 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "tweede_kamer = pd.read_csv(\"CorpusTweedeKamer.zip\")\n",
    "tweede_kamer = (\n",
    "    tweede_kamer\n",
    "    .assign(datum = pd.to_datetime(tweede_kamer.date))\n",
    "    .assign(speaker = tweede_kamer.speaker.str.lower())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = tweede_kamer.query(\"party == 'VVD'\").sample(1).text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \" \".join(sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'De argumenten zijn al gegeven in de schriftelijke inbreng; daar staan zij aangegeven. Wij hebben allen, zo mag ik aannemen, veel reacties gekregen en gesprekken gevoerd met vakbeweging, met werkgevers, met organisaties op dit terrein. Wij kennen de argumenten. U vraagt almaar door naar de bekende weg. Nogmaals: ik ga het niet uitlokken, ik wacht niet tot het gebeurd is. Dan zitten wij namelijk hier bij elkaar en zeggen wij: tjonge, wat erg dat dit toch gebeurd is.'"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(inputs, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 99])"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = input_ids.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_output = model.generate(input_ids, max_length= N +20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "De argumenten zijn al gegeven in de schriftelijke inbreng; daar staan zij aangegeven. Wij hebben allen, zo mag ik aannemen, veel reacties gekregen en gesprekken gevoerd met vakbeweging, met werkgevers, met organisaties op dit terrein. Wij kennen de argumenten. U vraagt almaar door naar de bekende weg. Nogmaals: ik ga het niet uitlokken, ik wacht niet tot het gebeurd is. Dan zitten wij namelijk hier bij elkaar en zeggen wij: tjonge, wat erg dat dit toch gebeurd is.\n--------------\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'dat is niet zo erg, hoor. Nogmaals : wij vragen het niet meer meer terug'"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(\"--------------\")\n",
    "out = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "zz = len(out)\n",
    "out[tt:zz]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['cls.predictions.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "tokenizer_NL = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model_NL = AutoModelWithLMHead.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model = model_NL, tokenizer= tokenizer_NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               date  agenda  speechnumber                         speaker  \\\n0        1994-12-20     NaN             1                     marijnissen   \n1        1994-12-20     NaN             2                         melkert   \n2        1994-12-20     NaN             3                     marijnissen   \n3        1994-12-20     NaN             4                         melkert   \n4        1994-12-20     NaN             5                     rosenmöller   \n...             ...     ...           ...                             ...   \n1143361  2019-07-04     NaN           965  staatssecretaris broekers-knol   \n1143362  2019-07-04     NaN           966                   de voorzitter   \n1143363  2019-07-04     NaN           967                   de voorzitter   \n1143364  2019-07-04     NaN           968          mevrouw van toorenburg   \n1143365  2019-07-04     NaN           969                   de voorzitter   \n\n        party  party.facts.id  chair  terms  \\\n0          SP          1363.0  False    561   \n1        PvdA          1234.0  False    706   \n2          SP          1363.0  False    304   \n3        PvdA          1234.0  False    374   \n4          GL          1537.0  False    412   \n...       ...             ...    ...    ...   \n1143361   NaN             NaN  False    142   \n1143362   NaN             NaN   True      3   \n1143363   NaN             NaN   True     62   \n1143364   CDA          1157.0  False     21   \n1143365   NaN             NaN   True     75   \n\n                                                      text      parliament  \\\n0        Mijnheer de voorzitter! Ik vertel de minister ...  NL-TweedeKamer   \n1        Mijnheer de voorzitter! Mag ik allereerst de h...  NL-TweedeKamer   \n2        Mijnheer de voorzitter! Hoewel ik het antwoord...  NL-TweedeKamer   \n3        Mijnheer de voorzitter! Wat is onrechtvaardig?...  NL-TweedeKamer   \n4        Voorzitter! Afgelopen zaterdag stond in NRC Ha...  NL-TweedeKamer   \n...                                                    ...             ...   \n1143361  Dan de motie op stuk nr. 2519 van de heer Hidd...  NL-TweedeKamer   \n1143362                                        Dank u wel.  NL-TweedeKamer   \n1143363  Over exact 60 minuten gaan wij stemmen over de...  NL-TweedeKamer   \n1143364  Toch nog even — misschien is het allemaal afge...  NL-TweedeKamer   \n1143365  Ja, wat mij betreft zou ik 60 seconden willen ...  NL-TweedeKamer   \n\n        iso3country      datum  \n0               NLD 1994-12-20  \n1               NLD 1994-12-20  \n2               NLD 1994-12-20  \n3               NLD 1994-12-20  \n4               NLD 1994-12-20  \n...             ...        ...  \n1143361         NLD 2019-07-04  \n1143362         NLD 2019-07-04  \n1143363         NLD 2019-07-04  \n1143364         NLD 2019-07-04  \n1143365         NLD 2019-07-04  \n\n[1143366 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>agenda</th>\n      <th>speechnumber</th>\n      <th>speaker</th>\n      <th>party</th>\n      <th>party.facts.id</th>\n      <th>chair</th>\n      <th>terms</th>\n      <th>text</th>\n      <th>parliament</th>\n      <th>iso3country</th>\n      <th>datum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1994-12-20</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>marijnissen</td>\n      <td>SP</td>\n      <td>1363.0</td>\n      <td>False</td>\n      <td>561</td>\n      <td>Mijnheer de voorzitter! Ik vertel de minister ...</td>\n      <td>NL-TweedeKamer</td>\n      <td>NLD</td>\n      <td>1994-12-20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1994-12-20</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>melkert</td>\n      <td>PvdA</td>\n      <td>1234.0</td>\n      <td>False</td>\n      <td>706</td>\n      <td>Mijnheer de voorzitter! Mag ik allereerst de h...</td>\n      <td>NL-TweedeKamer</td>\n      <td>NLD</td>\n      <td>1994-12-20</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1994-12-20</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>marijnissen</td>\n      <td>SP</td>\n      <td>1363.0</td>\n      <td>False</td>\n      <td>304</td>\n      <td>Mijnheer de voorzitter! Hoewel ik het antwoord...</td>\n      <td>NL-TweedeKamer</td>\n      <td>NLD</td>\n      <td>1994-12-20</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1994-12-20</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>melkert</td>\n      <td>PvdA</td>\n      <td>1234.0</td>\n      <td>False</td>\n      <td>374</td>\n      <td>Mijnheer de voorzitter! Wat is onrechtvaardig?...</td>\n      <td>NL-TweedeKamer</td>\n      <td>NLD</td>\n      <td>1994-12-20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1994-12-20</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>rosenmöller</td>\n      <td>GL</td>\n      <td>1537.0</td>\n      <td>False</td>\n      <td>412</td>\n      <td>Voorzitter! Afgelopen zaterdag stond in NRC Ha...</td>\n      <td>NL-TweedeKamer</td>\n      <td>NLD</td>\n      <td>1994-12-20</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1143361</th>\n      <td>2019-07-04</td>\n      <td>NaN</td>\n      <td>965</td>\n      <td>staatssecretaris broekers-knol</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>142</td>\n      <td>Dan de motie op stuk nr. 2519 van de heer Hidd...</td>\n      <td>NL-TweedeKamer</td>\n      <td>NLD</td>\n      <td>2019-07-04</td>\n    </tr>\n    <tr>\n      <th>1143362</th>\n      <td>2019-07-04</td>\n      <td>NaN</td>\n      <td>966</td>\n      <td>de voorzitter</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>3</td>\n      <td>Dank u wel.</td>\n      <td>NL-TweedeKamer</td>\n      <td>NLD</td>\n      <td>2019-07-04</td>\n    </tr>\n    <tr>\n      <th>1143363</th>\n      <td>2019-07-04</td>\n      <td>NaN</td>\n      <td>967</td>\n      <td>de voorzitter</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>62</td>\n      <td>Over exact 60 minuten gaan wij stemmen over de...</td>\n      <td>NL-TweedeKamer</td>\n      <td>NLD</td>\n      <td>2019-07-04</td>\n    </tr>\n    <tr>\n      <th>1143364</th>\n      <td>2019-07-04</td>\n      <td>NaN</td>\n      <td>968</td>\n      <td>mevrouw van toorenburg</td>\n      <td>CDA</td>\n      <td>1157.0</td>\n      <td>False</td>\n      <td>21</td>\n      <td>Toch nog even — misschien is het allemaal afge...</td>\n      <td>NL-TweedeKamer</td>\n      <td>NLD</td>\n      <td>2019-07-04</td>\n    </tr>\n    <tr>\n      <th>1143365</th>\n      <td>2019-07-04</td>\n      <td>NaN</td>\n      <td>969</td>\n      <td>de voorzitter</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>75</td>\n      <td>Ja, wat mij betreft zou ik 60 seconden willen ...</td>\n      <td>NL-TweedeKamer</td>\n      <td>NLD</td>\n      <td>2019-07-04</td>\n    </tr>\n  </tbody>\n</table>\n<p>1143366 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "tweede_kamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lange_tekst = (\n",
    "    tweede_kamer\n",
    "    .query(\"terms > 100\")\n",
    "    .query(\"terms < 512\")\n",
    "    .query(\"party == 'VVD'\")\n",
    "    .sample(1)\n",
    "    .text.values.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Gisteren heb ik ook heel duidelijk gezegd dat altijd het \"h-woord\" genoemd wordt in geval van hervormingen. Ik heb ook aangegeven wat het vorige kabinet daaraan gedaan heeft en dat het nu tijd wordt om eveneens iets aan het huurstelsel te doen. Gelukkig hoorde ik de minister zeggen dat daar ook naar gekeken wordt. De minister kan wel aangeven dat het niet rechtvaardig is, meer ondersteund te worden of om meer hypotheekrenteaftrek te krijgen als je meer verdient. Ik wijs er echter wel op dat mensen die meer verdienen, meer belasting betalen. Wij zullen wat dat betreft een evenwicht moeten vinden. Ik meen dat het huidige kabinet ook het eigen woningforfait voor de echt dure huizen al heeft verhoogd. Volgens mij is het bedrag echter niet geïndexeerd. Uiteindelijk zijn wij wat dat betreft dus ook allemaal aan de beurt.'"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "lange_tekst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "The context has 208 number of tokens, but `max_length` is only 200. Please make sure that `max_length` is bigger than the number of tokens, by setting either `generate(max_length=...,...)` or `config.max_length = ...`",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1173611c1611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlange_tekst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, return_tensors, return_text, clean_up_tokenization_spaces, *documents, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m             summaries = self.model.generate(\n\u001b[0;32m-> 1539\u001b[0;31m                 \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1540\u001b[0m             )\n\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m         assert (\n\u001b[1;32m    432\u001b[0m             \u001b[0mcur_len\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         ), f\"The context has {cur_len} number of tokens, but `max_length` is only {max_length}. Please make sure that `max_length` is bigger than the number of tokens, by setting either `generate(max_length=...,...)` or `config.max_length = ...`\"\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The context has 208 number of tokens, but `max_length` is only 200. Please make sure that `max_length` is bigger than the number of tokens, by setting either `generate(max_length=...,...)` or `config.max_length = ...`"
     ]
    }
   ],
   "source": [
    "summarizer(lange_tekst[0], min_length=10, max_length = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}